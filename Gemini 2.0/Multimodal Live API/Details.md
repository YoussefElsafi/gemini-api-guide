--------------------------------------------------------------------------------

                      Gemini API: Multimodal Live API

--------------------------------------------------------------------------------

OVERVIEW:

The Multimodal Live API enables *real-time, bidirectional* interactions with the Gemini model using voice and video.  It's designed for low-latency communication, providing a natural, conversational experience.  Users can interrupt the model's responses, mimicking human conversation.  The API handles text, audio, and video input, and it can output text and audio.

KEY CAPABILITIES:

*   **Multimodality:**  The model processes input from multiple modalities: seeing (video), hearing (audio), and reading (text).  It can respond with text and audio.
*   **Low-Latency, Real-Time Interaction:** Designed for fast responses, making conversations feel natural.
*   **Session Memory:**  The model remembers the context of the *current* session, recalling previous inputs (text, audio, video).  It does *not* store long-term conversation history.
*   **Tool Integration:** Supports:
    *   **Function Calling:** Connects to external services.
    *   **Code Execution:** Runs code to perform tasks.
    *   **Search as a Tool:**  Integrates search capabilities.
*   **Automated Voice Activity Detection (VAD):**  Automatically detects when a user starts and stops speaking, enabling interruptions and natural turn-taking. VAD is *always on* and *not configurable*.

GETTING STARTED:

*   **Stateful API:**  The Multimodal Live API uses WebSockets to maintain a persistent connection (a "session") between the client and the Gemini server.
*   **Example (Text-to-Text):**  The provided article demonstrates basic text-to-text interaction using Python 3.9+.  It shows how to:
    *   Install the `google-genai` library.
    *   Establish a WebSocket connection.
    *   Send text messages to the model.
    *   Receive text responses from the model.

INTEGRATION GUIDE:

1.  **Sessions:**

    *   A WebSocket connection establishes a session.
    *   Messages are exchanged within this session to send input and receive output.
    *   The *first message* after connecting must include the *session configuration*:
        *   `model`: The Gemini model to use (e.g., `gemini-2.0-flash-exp`).
        *   `generationConfig`: Parameters controlling text generation (e.g., `temperature`, `maxOutputTokens`).
        *   `systemInstruction`:  Optional instructions to guide the model's overall behavior and tone.
        *   `tools`:  Definitions of any functions the model can call.
        *   `responseModalities`: Specifies the type of output, example: "TEXT"

2.  **Sending Messages (Client -> Server):**

    *   Messages are JSON objects.
    *   A client message MUST contain *exactly one* of the following fields:
        *   `setup`:  (BidiGenerateContentSetup) - The initial session configuration.  Sent only once, at the beginning.
        *   `clientContent`: (BidiGenerateContentClientContent) - Incremental text updates to the conversation (e.g., establishing context, sending user text input). *Not* for function call responses.
        *   `realtimeInput`: (BidiGenerateContentRealtimeInput) - Real-time audio or video input.  Can be sent continuously, even while the model is generating output.
        *   `toolResponse`: (BidiGenerateContentToolResponse) - Responses to function calls made by the model.

3.  **Receiving Messages (Server -> Client):**

    *   Listen for the WebSocket `message` event.
    *   Parse the received data. It will be either:
        *   A `Blob` (for audio/video data).
        *   A JSON object.
    *   A server JSON message MUST contain *exactly one* of the following fields:
        *   `setupComplete`: (BidiGenerateContentSetupComplete) - Confirmation that the session setup is complete.
        *   `serverContent`: (BidiGenerateContentServerContent) - Content generated by the model (text or audio).
        *   `toolCall`: (BidiGenerateContentToolCall) - A request for the client to execute a function call.
        *   `toolCallCancellation`: (BidiGenerateContentToolCallCancellation) - Notification that a previous function call should be canceled (due to interruption).

4.  **Incremental Content Updates (`clientContent`):**

    *   Used for sending text input, establishing context, or restoring a previous session's context.
    *   For short contexts, send turn-by-turn interactions.
    *   For longer contexts, send a summary to save space in the context window.
    *   `BidiGenerateContentClientContent` is *not* used for responses to function calls; use `BidiGenerateContentToolResponse` instead.

5.  **Streaming Audio and Video (`realtimeInput`):**

    *   Use `BidiGenerateContentRealtimeInput` for streaming audio/video.
    *   Can be sent continuously without interrupting model generation.
    *   The model automatically detects the start and end of speech (VAD).
    *   Data is processed incrementally to minimize latency.

6.  **Function Calling:**

    *   All functions MUST be declared at the *start* of the session within the `BidiGenerateContentSetup` message.
    *   The model can generate multiple function calls and the code to chain their outputs.
    *   Code execution happens in a sandbox.
    *   Execution pauses until function call results are available (sequential processing).
    *   The client responds with `BidiGenerateContentToolResponse`.
    *   *Audio input/output negatively impacts the model's ability to use function calling.*

7.  **Audio Formats:**

    *   **Input:** Raw 16-bit PCM audio at 16kHz, little-endian.
    *   **Output:** Raw 16-bit PCM audio at 24kHz, little-endian.

8.  **System Instructions:**

    *   Provide instructions to control the model's output, tone, and sentiment of audio responses.
    *   Set *only at the beginning* of the session.
    *   Use incremental content updates (`clientContent`) for further input during the session.

9.  **Interruptions:**

    *   Users can interrupt the model's output at any time.
    *   VAD detects interruptions.
    *   Ongoing generation is canceled and discarded.
    *   The server sends a `BidiGenerateContentServerContent` message indicating the interruption.
    *   Pending function calls are also canceled, and a `BidiGenerateContentToolCallCancellation` message is sent.

10. **Voices:**

    *   Supported voices: Aoede, Charon, Fenrir, Kore, and Puck.
    *   Specify the voice using the `voiceName` within the `speechConfig` object in the session configuration.

LIMITATIONS:

*   **Client Authentication:**  The API is designed for *server-to-server* authentication.  It's *not recommended* for direct client use.  Client input should be routed through an intermediate application server for security.
*   **Conversation History:**  The model tracks in-session interactions, but *does not store* conversation history.  Context is lost when a session ends.  Applications must manage their own conversation logs and send them at the start of a new session using `BidiGenerateContentClientContent`.
*   **Session Duration:**
    *   Audio-only: Up to 15 minutes.
    *   Audio and Video: Up to 2 minutes.
    *   Exceeding the limit terminates the connection.
    *   Large content chunks can also cause early termination due to context size limits.
*   **Voice Activity Detection (VAD):**  Always enabled; parameters are *not configurable*.
*   **Token Count:**  Token counting is *not supported*.
*   **Rate Limits:**
    *   3 concurrent sessions per API key.
    *   4M tokens per minute.

MESSAGE AND EVENT DETAILS:

The document provides detailed descriptions of the following messages and their fields:

*   `BidiGenerateContentClientContent`
*   `BidiGenerateContentRealtimeInput`
*   `BidiGenerateContentServerContent`
*   `BidiGenerateContentSetup`
*   `BidiGenerateContentSetupComplete`
*   `BidiGenerateContentToolCall`
*   `BidiGenerateContentToolCallCancellation`
*   `BidiGenerateContentToolResponse`
*   Links to documentation for common types (Blob, Content, FunctionCall, etc.)

This `details.md` provides a comprehensive summary of the Multimodal Live API, covering its capabilities, how to use it, integration details, limitations, and a breakdown of the message types. It is suitable for developers who want to understand the API and build real-time, interactive applications with Gemini.